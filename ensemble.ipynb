{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dfd8ae6",
   "metadata": {},
   "source": [
    "# Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c181c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b66ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples = 500, noise = 0.30, random_state = 42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af5a5369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(random_state=42)),\n",
       "                             ('rf', RandomForestClassifier(random_state=42)),\n",
       "                             ('svc', SVC(random_state=42))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('lr', LogisticRegression(random_state =42)),\n",
    "                 ('rf', RandomForestClassifier(random_state = 42)),\n",
    "                 ('svc', SVC(random_state=42))]\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0abc1cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.864\n",
      "rf = 0.896\n",
      "svc = 0.896\n"
     ]
    }
   ],
   "source": [
    "for name, clf in voting_clf.named_estimators_.items():\n",
    "    print(name, \"=\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e2a4461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting classes (hard voting) - voting based on largest group of predictions\n",
    "\n",
    "voting_clf.predict(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d11a9e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1]), array([1]), array([0])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[clf.predict(X_test[:1]) for clf in voting_clf.estimators_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d9f1fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scoring\n",
    "voting_clf.score(X_test, y_test) #outperforms all individual classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd7a9059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#soft voting - voting based on avg probabilites \n",
    "voting_clf.voting = \"soft\"\n",
    "voting_clf.named_estimators[\"svc\"].probability = True\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_clf.score(X_test, y_test) #higher than hard voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd00240",
   "metadata": {},
   "source": [
    "## Bagging (bootstrap aggregating) and pasting\n",
    "* training same algorithms on different subsets of data (with replacement) - bagging\n",
    "* training same algorithms on different subsets of data (w/o replacement) - pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03b2793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "427c2efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators = 500,\n",
    "                           max_samples = 100, n_jobs = -1, random_state = 42)\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bf111f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.score(X_test, y_test) #bagging has slightly higher bias than pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a7f358",
   "metadata": {},
   "source": [
    "### out of bag evaluation\n",
    "Mathematically only about 63% of the training instances are sampled on an average for each predictor. The remaining 37% of the training instances that are not sampled are called out-of-bag(OOB) instances.\n",
    "\n",
    "A bagging ensemble can be evaluated using OOB instances, without the need for a separate validation se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee1663db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, \n",
    "                           oob_score = True, n_jobs = -1, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_ #classifier is likely to achieve about 89.6% accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d04b6254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8e14e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, bag_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f9df330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32352941, 0.67647059],\n",
       "       [0.3375    , 0.6625    ],\n",
       "       [1.        , 0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OOB decision function (because underlying algo has predict_proba() method)\n",
    "bag_clf.oob_decision_function_[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac6dea",
   "metadata": {},
   "source": [
    "## Random patches and Random subspaces\n",
    "\n",
    "just as we choose subset of instances for each estimator, we can choose features instead with `max_features` and `bootstrap_features` hyperparameters.\n",
    "\n",
    "Sampling both training instances and features is called the `random patches method`\n",
    "\n",
    "Keeping all training instances (by setting `bootstrap=False` and `max_samples=1.0`) but sampling features (by setting bootstrap_features to True and/or max_features to a value smaller than 1.0) is called the `random subspaces` method.⁠"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8444dd14",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "* same as training an ensemble classifier with decision trees via bagging method <br>\n",
    "* at each node only a random subset of the 'best' features is considered for splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12b5e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2777b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16,\n",
    "                                n_jobs = -1, random_state = 42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4713951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8133807",
   "metadata": {},
   "source": [
    "by default uses $\\sqrt{n}$ features out of n.<br>\n",
    "Trades higher bias for a lower variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb58332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(max_features = \"sqrt\", max_leaf_nodes = 16),\n",
    "                           n_estimators = 500, n_jobs = -1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f5a974",
   "metadata": {},
   "source": [
    "### extremely random trees (extra-trees)\n",
    "* random forest with splitting nodes using random thresholds for each features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d4c87fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra trees; making trees even more random \n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f85736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_clf = ExtraTreesClassifier(random_state=42,)\n",
    "extra_clf.fit(X_train, y_train)\n",
    "y_pred_extra  = extra_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b01b5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_extra, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63574720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestClassifier(bootstrap=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "341518a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_clf = BaggingClassifier(DecisionTreeClassifier(max_features = \"sqrt\", max_leaf_nodes = 16, splitter='random'),\n",
    "                           n_estimators = 500, n_jobs = -1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cbd7860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_clf.fit(X_train, y_train)\n",
    "accuracy_score(y_test, ex_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023466de",
   "metadata": {},
   "source": [
    "### feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "115375c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=500, random_state=42)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris(as_frame = True)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, random_state = 42)\n",
    "rnd_clf.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a6f0615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) : 0.11\n",
      "sepal width (cm) : 0.02\n",
      "petal length (cm) : 0.44\n",
      "petal width (cm) : 0.42\n"
     ]
    }
   ],
   "source": [
    "for score,name in zip(rnd_clf.feature_importances_,iris.data.columns):\n",
    "    print(name, \":\", round(score,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c89e4b5",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting (originally called hypothesis boosting) refers to any Ensemble method that\n",
    "can combine several weak learners into a strong learner. The general idea of most\n",
    "boosting methods is to train predictors sequentially, each trying to correct its predecessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9fe490",
   "metadata": {},
   "source": [
    "### 1. Adaptive Boosting (AdaBoost)\n",
    "\n",
    "One way for a new predictor to correct its predecessor is to pay a bit more attention\n",
    "to the training instances that the predecessor underfitted. This results in new predic‐\n",
    "tors focusing more and more on the hard cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a98d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stagewise additive modeling using a multiclass Exponential loss function\n",
    "# SAMME and SAMME.R\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d320084",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(\n",
    "DecisionTreeClassifier(max_depth = 1), n_estimators= 200,\n",
    "algorithm = \"SAMME.R\", learning_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a5f684c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c09de",
   "metadata": {},
   "source": [
    "### 2. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cbcf56e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f2cf17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "594ceb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg1 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg1.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06a53712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg2.fit(X,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d713323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec6ab701",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5cc7a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [sum(tree.predict(X_new)) for tree in (tree_reg1, tree_reg2, tree_reg3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "214e0b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3ff6f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 3, learning_rate = 1.0)\n",
    "gbrt.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a0f0cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the optimal number of estimators in Gradient Boost; early stopping\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "142e46de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1daad2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=120)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 120)\n",
    "gbrt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "852a8261",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "865eedfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_n_estimators = np.argmin(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca5c01dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=52)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_best = GradientBoostingRegressor(max_depth =2, n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a526a880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashishk/anaconda3/envs/ml/lib/python3.10/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# xgboost\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7dcdae23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "             gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.300000012,\n",
       "             max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a085697",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65021c03",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "be811bde",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StackingClassifier\n\u001b[1;32m      3\u001b[0m stacking_clf \u001b[38;5;241m=\u001b[39m StackingClassifier(\n\u001b[1;32m      4\u001b[0m     estimators\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      5\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m, LogisticRegression(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# number of cross-validation folds\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mstacking_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/ensemble/_stacking.py:485\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit the estimators.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m        Returns a fitted instance of estimator.\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m     \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le \u001b[38;5;241m=\u001b[39m LabelEncoder()\u001b[38;5;241m.\u001b[39mfit(y)\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mclasses_\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/utils/multiclass.py:198\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    190\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    197\u001b[0m ]:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('svc', SVC(probability=True, random_state=42))\n",
    "    ],\n",
    "    final_estimator=RandomForestClassifier(random_state=43),\n",
    "    cv=5  # number of cross-validation folds\n",
    ")\n",
    "stacking_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ad06da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
