# learn-ML
Introduction to Machine learning (based on [CS229](https://cs229.stanford.edu/) from stanford university)<br>

<ul>
	<li> <a href="https://cs229.stanford.edu/"> Course website </a> </li>
<li><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU"> Video lectures </a> </li>
<li><a href="https://see.stanford.edu/Course/CS229"> vintage version of cs229 </a>  </li>
</ul>

<table>
<tr>
<th> Concept </th>
<th> lecture </th>
<th> applications</th>
<th> extras </th>
</tr>
<tr>
	
<td> 
	<p> Linear models 
<ul>
	<li> <a href="ordinary_linear_reg_grad_descent.ipynb"> linear regression (gradient descent) </a> </li>
	<li>  <a href="ord_linear_reg_stochastic_GD.ipynb"> linear regression (stochastic GD) </a> </li>
	<li> <a href="Polynomial_regression.ipynb"> Polynomial regression </a> </li>
	<li> <a href="newton_raphson_log_reg.ipynb"> Logistic regression </a> </li>
	<li> <a href="multiclass_classification_softmax.ipynb"> Softmax regression(multiclass) </a> </li>
	<li> <a href="plot_poisson_regression_non_normal_loss.ipynb"> Poisson regression </a> </li>
	<li> <a href="plot_tweedie_regression_insurance_claims.ipynb"> GLM </a>	</li>
    </ul></p>
</td>
<td>
<ul>
<li> <a href="https://www.youtube.com/watch?v=4b4MUYve_U8&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=2"> ML problem setup </a> </li>
<li> <a href="https://www.youtube.com/watch?v=4b4MUYve_U8&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=2"> LR and GD </a> </li>
<li> <a href="https://www.youtube.com/watch?v=het9HFqo1TQ&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=3"> Weighted & Logistic regression </a> </li>
<li> <a href="https://www.youtube.com/watch?v=iZTeva0WSTQ&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=4"> GLMs & cross-entroy min </a> </li>

</ul>
 </td>
<td> <ul>
	<li> <a href="pred_diabetes_progress.ipynb"> Predicting diabetes progression</a> </li>
	</ul>
</td>
<td>
<ul>
<li> <a href="https://towardsdatascience.com/assumptions-of-linear-regression-fdb71ebeaa8b"> assumptions of LR </a> </li>
<li> <a href="https://dl.acm.org/doi/10.1162/neco.1996.8.7.1341"> No free lunch </a> </li> 
<li> <a href="https://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/35179.pdf"> data effects </a> </li>
<li> <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3164764"> case study: GLM </a> </li>
 </ul>
 </td>
</tr>
<tr> 
<td>	<ul>
<li> <a href="Naive_bayes.ipynb"> Naive Bayes </a> </li>
</ul>
</td>
<td>
<ul>
<li> <a href="https://www.youtube.com/watch?v=nt63k3bfXS0&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=5"> GDA and Naive Bayes </a> </li>
</ul>
</td>
<td>
<ul> <li> <a href="spam_classification.ipynb"> Spam classification using NB </a></li> </ul>
</td>
<td>
</td>
</tr>
<tr> 
 <td>
   <ul>
 <li> <a href="SVM_classifiers.ipynb">Support Vector Machines </a></li>
   </ul>
 </td>
 <td>
  <ul>
   <li> <a href="https://www.youtube.com/watch?v=lDwow4aOrtg&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=6"> Support Vector Machines </a> </li>
   <li> <a href="https://www.youtube.com/watch?v=8NYoQiRANpg&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=7"> Kernels </a> </li>
</ul>
</td>
<td> <ul> <li> <a href="MNIST_svm.ipynb"> Digit recognition (clf) </a> </li>
<li> <a href="Predicting_house_prices.ipynb"> Housing price prediction (reg) </a> </li> </ul>
</td>
<td> <ul> <li> <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf"> SMO algorithm </a> </li>
<li> <a href="https://dl.acm.org/doi/10.1145/1390156.1390208"> Dual coordinate descent </a></li>
<li> <a href="https://proceedings.neurips.cc/paper/2000/file/155fa09596c7e18e50b58eb7e0c6ccb4-Paper.pdf"> Online learning </a> </li>
<li> <a href="https://jmlr.org/papers/volume6/bordes05a/bordes05a.pdf"> Fast Kernels </a> </li>
</ul>
</td>
</tr>
<tr>
<td>  <p> Nonparametric methods
<ul> <li> <a href="decision_trees.ipynb"> Decision Trees </a> </li>
<li> <a href="ensemble.ipynb"> Ensemble learning </a> </li>
 </ul>
 </p></td>
<td> <a href="https://youtu.be/wr9gUr-eWdA"> DT & Ensemble methods </a></td>
<td> <a href="MNIST_ensemble.ipynb"> MNIST ensemble </a> </td>
<td> </td>
 </tr>
<tr>
<td> <p> Learning theory
<ul> <li> <a href="https://cs229.stanford.edu/notes2022fall/bias-variance.pdf"> Bias, variance and ERM </a></li>
 </ul> </p> </td>
<td> <ul> <li> <a href="https://www.youtube.com/watch?v=rjbkWSTjHzM&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=8"> Data Split, Models & CV </a> </li>
<li> <a href="https://www.youtube.com/watch?v=iVOxMcumR4A&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=9"> Estimation Error and ERM </a> </li>
</ul>  </td>
<td> </td>
<td> <ul> <li> <a href="https://epubs.siam.org/doi/10.1137/20M1336072"> Double Descent phenomenon </a> </li> </ul> </td>
 </tr>
</table>
